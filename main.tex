\documentclass{beamer}
%
% Choose how your presentation looks.
%
% For more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
%
\mode<presentation>
{
  \usetheme{default}      % or try Darmstadt, Madrid, Warsaw, ...
  \usecolortheme{default} % or try albatross, beaver, crane, ...
  \usefonttheme{default}  % or try serif, structurebold, ...
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
  \setbeamertemplate{footline}[frame number]
} 

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{dirtree}
\usepackage{listings}
\usepackage{courier}
\usepackage{array}

\title[2016-04-04-offline-pr-overview]{Big Data techniques and \\ applying them to high energy physics}
\author{Jim Pivarski}
\institute{Princeton University -- DIANA}
\date{April 4, 2016}

\xdefinecolor{darkblue}{rgb}{0.1,0.1,0.7}
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\lstset{ %
  backgroundcolor=\color{white},   % choose the background color
  basicstyle=\ttfamily\scriptsize,        % size of fonts used for the code
  breaklines=true,                 % automatic line breaking only at whitespace
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  keywordstyle=\color{blue},       % keyword style
  stringstyle=\color{mymauve},     % string literal style
  showstringspaces=false,
  showlines=true
}

\lstdefinelanguage{scala}{
  morekeywords={abstract,case,catch,class,def,%
    do,else,extends,false,final,finally,%
    for,if,implicit,import,match,mixin,%
    new,null,object,override,package,%
    private,protected,requires,return,sealed,%
    super,this,throw,trait,true,try,%
    type,val,var,while,with,yield},
  otherkeywords={=>,<-,<\%,<:,>:,\#,@},
  sensitive=true,
  morecomment=[l]{//},
  morecomment=[n]{/*}{*/},
  morestring=[b]",
  morestring=[b]',
  morestring=[b]"""
}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

% Uncomment these lines for an automatically generated outline.
%\begin{frame}{Outline}
%  \tableofcontents
%\end{frame}

\begin{frame}{How did I end up giving this talk?}
\begin{itemize}
\item I'm a CMS physicist, contributed to muon alignment in Run-1 commissioning and early exotica searches.
\item Spent the next five years as a data science consultant:
\begin{itemize}
\item Helped commercial clients analyze data with Big Data technologies: Hadoop, Spark, NoSQL, etc.
\item Some statistics, but more software and data pipelines.
\end{itemize}
\end{itemize}

\includegraphics[width=\linewidth]{spectrum_of_data_science.png}

\uncover<2->{\fbox{\parbox{\linewidth}{Now I'm part of a project to introduce Big Data tools to HEP. \\ \centering \url{http://diana-hep.org}}}}
\end{frame}

\begin{frame}{Differences between HEP and the Big Data world}
\begin{columns}
\column{0.5\linewidth}
\centering \textcolor{darkblue}{HEP}

\column{0.5\linewidth}
\centering \textcolor{darkblue}{Big Data}
\end{columns}

\vfill
\begin{columns}
\column{0.5\linewidth}
One integrated toolkit: ROOT. Compared to a ``city with central services'' in the User's Guide.

\column{0.5\linewidth}
Many competing frameworks and data formats that interoperate: ``city-states?''
\end{columns}

\vfill
\uncover<2->{\hrulefill}

\vfill
\begin{columns}<2->
\column{0.5\linewidth}
C++ for data plumbing and (increasingly) Python for end-user analysis.

\column{0.5\linewidth}
Java for data plumbing and (mostly) Python, R, and SQL for analysis.
\end{columns}

\vfill
\uncover<3->{\hrulefill}

\vfill
\begin{columns}<3->
\column{0.5\linewidth}
Extensive distributed filtering \\ and data transformations \\ (the ``map'' of map-reduce).

\column{0.5\linewidth}
Extensive distributed map {\it and} reduce: regrouping data from one lookup key to another.
\end{columns}

\vfill
\uncover<4->{\hrulefill}

\vfill
\begin{columns}<4->
\column{0.5\linewidth}
Data analysis mostly written imperatively: for loops, break/continue statements.

\column{0.5\linewidth}
Increasing use of functional primitives: map, filter, flatMap, reduce, aggregate.
\end{columns}
\end{frame}

\vfill
\begin{frame}{The ecosystem}
\vspace{-0.6 cm}
\includegraphics[width=\linewidth]{word-cloud-07-2014.png}

\begin{onlyenv}<2->
\vspace{0.5 cm}
\begin{columns}
\column{0.5\linewidth}
KDNuggets poll results:

\vspace{0.1 cm}
{\scriptsize
\begin{itemize}
\item Hadoop, 18.4\% share (507 votes)
\item Spark, 11.3\% (311)
\item Hive, 10.2\% (282)
\item SQL on Hadoop tools, 7.2\% (198)
\item Others, 21.4\% (592)
\end{itemize}}

\column{0.5\linewidth}
Google Trends search popularity:

\vspace{0.5 cm}
\includegraphics[width=\linewidth]{trends.png}
\end{columns}
\end{onlyenv}
\end{frame}

\begin{frame}{Diversity: just to make a point}

There are FIVE stream-processing systems maintained by Apache:

\begin{center}
\begin{minipage}{0.8\linewidth}
\begin{center}
\includegraphics[width=0.4\linewidth]{streaming-1.png} \hfill
\includegraphics[width=0.4\linewidth]{streaming-2.png}

\vspace{0.2 cm}
\includegraphics[width=0.5\linewidth]{streaming-3.png}

\vspace{0.2 cm}
\includegraphics[width=0.4\linewidth]{streaming-4.png} \hfill
\includegraphics[width=0.4\linewidth]{streaming-5.jpg}
\end{center}
\end{minipage}
\end{center}

\vfill
They all do the same thing (as far as I can tell). One will emerge as the most popular and get the most contributions (either Storm or Spark-Streaming).
\end{frame}

\begin{frame}{File formats: more ways of doing the same thing}

\vfill
\hspace{-0.4 cm}\begin{minipage}{\linewidth}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{p{0.4\linewidth} p{0.6\linewidth}}
\textcolor{darkblue}{JSON, XML, CSV\ldots} & Text-based for human readability; more common than you'd think. \\
\raggedright \textcolor{darkblue}{Avro, Thrift, Protocol buffers\ldots} & Generic, row-based binary data, like JSON with a schema. \\
\textcolor{darkblue}{Parquet, ORC, RCFile\ldots} & Generic, columnar binary data, used for SQL-style analyses. \\
\raggedright \textcolor{darkblue}{Java serialization, Kryo, Hadoop Writables\ldots} & Intermediate serialization for data in a distributed stream. \\
\textcolor{darkblue}{Python pickle files} & Persist Python objects between sessions. \\
\textcolor{darkblue}{RData, RDS} & Persist R objects between sessions. \\
\end{tabular}
\end{minipage}

\vfill
We can see this as an opportunity to do performance comparisons.

\vfill
\end{frame}

\begin{frame}{}
\begin{center}
\begin{minipage}{0.8\linewidth}
It's obvious why HEP hasn't chosen this model: who could maintain so many alternatives?

\vspace{1.5 cm}
But perhaps we can use what's out there and let the Big Data community maintain it.

\vspace{1.5 cm}
The major tools are available through package managers (Maven Central, PyPI, CRAN, \ldots), and there are third-party adapters between most pairs.
\end{minipage}
\end{center}
\end{frame}

\begin{frame}{The C++/Java barrier}

\vspace{0.2 cm}
One thing most of these toolkits have in common is that they are written for the Java Virtual Machine (JVM). This is hard to bridge.

\begin{center}
\includegraphics[width=0.5\linewidth]{three_bears.png}
\end{center}

\renewcommand{\arraystretch}{1.2} \begin{tabular}{>{\raggedright}p{0.23\linewidth} >{\raggedright}p{0.35\linewidth} >{\raggedright\arraybackslash}p{0.33\linewidth}}
\textcolor{darkblue}{C/C++} & \textcolor{darkblue}{JVM (Java, Scala\ldots)} & \textcolor{darkblue}{Python and R} \\
\vspace{-0.4 cm} Fast, but hard to debug. & \vspace{-0.4 cm} Type-aware bytecode is moderately fast. & \vspace{-0.4 cm} Offload heavy work to precompiled packages. \\
Mixes analysis with hardware concerns. & High-level abstractions for data analysis, but better geared toward batch processes. & Highly interactive; ideal the compute- plot-think cycle.
\end{tabular}
\end{frame}

\begin{frame}{}
\vfill
\includegraphics[width=\linewidth]{language_trends.png}

\vfill
Warning: trend analyses differ by $\sim$15\%.
\end{frame}

\begin{frame}{}
\vfill
More specifically: what languages are used for data analytics?

\includegraphics[width=\linewidth]{KDNuggetsPoll.pdf}

\vfill
\scriptsize (2013 KDNuggets poll)
\end{frame}

\begin{frame}{Overcoming the C++/Java barrier}

\begin{itemize}
\item Python and R have extension modules to call compiled code.
\item Java has its JNI (Java Native Interface), but it is {\it much less} used. (Except among high-frequency traders.)
\end{itemize}

\begin{uncoverenv}<2->
\begin{block}{}
\vspace{-\baselineskip}
I have been developing adapters to connect ROOT with the JVM so that we can use Big Data tools in our physics analyses.

\begin{itemize}
\item Using JNI so that C++ code and Java run in the same process (no interprocess communication).
\item Arbitrarily complex ROOT TTrees to include CMSSW objects.
\item ROOT does file I/O to inherit XRootD abilities.
\item Scala interface, rather than pure Java, to target Spark.
\item Scala macros ``compile in'' data type information for speed.
\item Combining Scala macros with ROOT's Cling interface to create Java interfaces with C++ implementations.
\end{itemize}
\end{block}
\end{uncoverenv}
\end{frame}

\begin{frame}{ScaROOT: Java interfaces with C++ implementations}

\vfill
\fbox{\url{http://github.com/diana-hep/scaroot}}

\vfill
\includegraphics[width=\linewidth]{wiki.png}
\end{frame}

\begin{frame}[fragile]{ScaROOT: Java interfaces with C++ implementations}
\begin{lstlisting}[language=scala]
import org.dianahep.scaroot.RootClass

// Java interface (called a ``trait'' in Scala):
trait ChiSqProb {
  def apply(chi2: Double, ndof: Int): Double
}

// C++ class definition that satisfies it:
val chiSqProbClass = RootClass[ChiSqProb]("""
class ChiSqProb {
public:
  double apply(double chi2, int ndof) {
    return ROOT::Math::chisquared_cdf(chi2, ndof);
  }
};
""")

// Create an instance:
val chiSqProb = chiSqProbClass.newInstance

// And use it:
chiSqProb.apply(53.8, 50)   // or just chiSqProb(53.8, 50)
                            // because 'apply' is 'operator()'
0.6689797343068249
\end{lstlisting}
\end{frame}

\begin{frame}[fragile]{ScaROOT-Reader: accessing ROOT TTrees}
\vfill
\fbox{\url{http://github.com/diana-hep/rootconverter}}

\vfill
\begin{minipage}{1.05\linewidth}
\begin{lstlisting}[language=scala]
// Define a concrete class for data to go into:
case class TwoMuon(mass: Float, px: Float, py: Float, pz: Float) {
  def pt = Math.sqrt(px*px + py*py)
}

// The brackets (equivalent of C++ template) generates custom
// data-filling code with a compile-time macro.
val iterator = RootTreeIterator[TwoMuon](
  List("root://cmseos.fnal.gov/TwoMuonNtuple.root"), "twoMuon")

// Do anything with this data stream in the JVM.
while (iterator.hasNext) {
  val datum = iterator.next()
  println(s"mass: ${datum.mass} pt: ${datum.pt}")
}
\end{lstlisting}
\end{minipage}

Also includes a pure-C++ {\tt root2avro} for streaming or bulk file conversion without involving the JVM.
\end{frame}

\begin{frame}[fragile]{Complex example: Bacon-tuple as an Avro schema}
\tiny
\begin{verbatim}
{"type": "record",
 "name": "Events",
 "fields": [
   {"name": "GenParticle", "type": {"type": "array", "items": {"type": "record",
    "name": "TGenParticle",
    "namespace": "baconhep",
    "fields": [
      {"name": "parent", "type": "int"},
      {"name": "pdgId", "type": "int"},
      {"name": "status", "type": "int"},
      {"name": "pt", "type": "float"},
      {"name": "eta", "type": "float"},
      {"name": "phi", "type": "float"},
      {"name": "mass", "type": "float"},
      {"name": "y", "type": "float"}
    ]
   }}},
   {"name": "LHEWeight", "type": {"type": "array", "items": {"type": "record",
    "name": "TLHEWeight",
    "namespace": "baconhep",
    "fields": [
      {"name": "id", "type": "int", "doc": "parton flavor PDG ID"},
      {"name": "weight", "type": "float", "doc": "generator-level event weight"}
    ]
   }}},
   {"name": "Electron", "type": {"type": "array", "items": {"type": "record",
    "name": "TElectron",
    "namespace": "baconhep",
    "fields": [
      {"name": "pt", "type": "float", "doc": "kinematics"},
      {"name": "eta", "type": "float", "doc": "kinematics"},
      {"name": "phi", "type": "float", "doc": "kinematics"},
      ...
\end{verbatim}
\end{frame}

\begin{frame}[fragile]{}
\hspace{-0.6 cm}ScaROOT-Reader is being used in a CMS monojet analysis:
\begin{columns}
\column{0.6\linewidth}
\begin{itemize}
\item Oliver Gutsche
\item Matteo Cremonesi
\item Cristina Su\'arez (made plot at right from Bacon-tuple in Spark)
\end{itemize}

\vspace{0.5 cm}
Part of a wider Big Data-in-HEP effort:
\begin{itemize}
\item Alexey Svyatkovskiy (Princeton)
\item Saba Sehrish and Jim Kowalkowski (FNAL Scientific Computing)
\end{itemize}
\column{0.5\linewidth}
\includegraphics[width=\linewidth]{THE_PLOT.png}
\end{columns}

\vspace{0.5 cm}
\begin{uncoverenv}<2->
\hspace{-0.6 cm}Producing a distributed TTree iterator in Spark:

\begin{lstlisting}[language=scala]
val rdd = file_list.flatMap(files => RootTreeIterator[Tree](
  files.map(_.xrootd_url), tree_location, libs, class_links))

rdd.map(...).filter(...).reduce(...)
\end{lstlisting}
\end{uncoverenv}
\end{frame}

\begin{frame}{}
\vfill
\textcolor{darkblue}{\Large So we can run physics events through Big Data \mbox{tools.\hspace{-1 cm}}}

\vspace{0.3 cm}
\textcolor{darkblue}{\Large What does that buy us?}

\begin{uncoverenv}<2->
\begin{itemize}\setlength{\itemsep}{0.2 cm}
\item Code maintained by large open-source organizations like Apache.
\item More complex forms of distributed data processing, such as the ``reduce'' step in map-reduce.
\item Stream processing for data quality monitoring.
\item Higher data-handling abstractions, which analysts in other fields are already benefiting from.
\end{itemize}
\end{uncoverenv}
\end{frame}

\begin{frame}{}

\begin{block}{Most physics analyses}
Uncoupled: to process one event, you don't need to know anything about any other events.
\end{block}

\begin{block}{Most industrial problems}
Require large datasets to be transposed, regrouping distributed data from one lookup key to another.
\end{block}

\begin{uncoverenv}<2->
\begin{block}{Example}
\begin{itemize}
\item A customer buys diapers online; what other products should you recommend?
\item Analysis of diaper purchases across all customers reveals a correlation with milk; recommend milk.
\end{itemize}
\end{block}
\end{uncoverenv}
\end{frame}

\begin{frame}{}
\begin{block}{Example}
\begin{itemize}
\item A customer buys diapers online; what other products should you recommend?
\item Analysis of diaper purchases across all customers reveals a correlation with milk; recommend milk.
\end{itemize}
\end{block}

\vfill
\textcolor{darkblue}{Customer records need to be split and grouped by purchase items.}

\begin{center}
\includegraphics[width=0.9\linewidth]{distributed_data.png}
\end{center}
\end{frame}

\begin{frame}[fragile]{This was Google's problem: solved with map-reduce}

Must reindex data from words-in-webpages to \mbox{webpages-with-words.\hspace{-1 cm}}

\begin{columns}
\column{0.5\linewidth}
\begin{lstlisting}[language=python,frame=single]
def mapper(webpage):
    for word in webpage.split():
        yield (word, webpage)
    
\end{lstlisting}
\column{0.58\linewidth}
\begin{lstlisting}[language=python,frame=single]
def reducer(word, webpages):
   searchIndex[word] = set()
   for webpage in webpages:
       searchIndex[word].add(webpage)
\end{lstlisting}
\end{columns}

\begin{itemize}
\item Independent mappers transform input to $\langle$key, value$\rangle$ pair.
\item Each reducer is given {\it all} pairs with a unique key.
\item Map-reduce framework optimizes the sorting and merging required to collect unique keys.
\end{itemize}

\begin{center}
\includegraphics[width=0.65\linewidth]{mapreduce-diagram-by-ibm.png}
\end{center}
\end{frame}

\begin{frame}[fragile]{Physics application: alignment with tracks}

Must reindex data from residuals-on-tracks to \mbox{residuals-on-subdetectors.\hspace{-1 cm}}

\vfill
\begin{lstlisting}[language=python,frame=single]
def mapper(event):
    for track in event:
        for hit in track:
            key = hit.subdetector()
            residual = track.projection(hit) - hit.position()
            yield (key, residual)
\end{lstlisting}

\begin{lstlisting}[language=python,frame=single]
def reducer(subdetector, residuals):
    numer = 0
    denom = 0
    for residual in residuals:
        numer = numer + residual
        denom = denom + 1
    move(subdetector, numer/denom)  # shift by residual mean
\end{lstlisting}

\vfill
\begin{itemize}
\item Applies broadly to many alignment and calibration tasks (e.g. reindex from energy-in-$\pi^0$ to energy-in-crystal).
\item Currently, detector groups are reimplementing this in custom ways, using tools designed for independent event processing.
\end{itemize}
\end{frame}

\begin{frame}{From Hadoop to Spark}
\begin{block}{2004}
Google publishes {\it MapReduce: Simplified Data Processing on Large Clusters.}
\end{block}

\begin{block}{2006}
Hadoop project started, mainly by Yahoo! but within Apache open-source.
\end{block}

\begin{block}{2008--2009}
Hadoop sorts TB--PB of data in record time. Starts getting contributions from Facebook, LinkedIn, eBay, and IBM.
\end{block}

\begin{block}{2009}
Spark begins as a class project at Berkley, targeting {\it iterative} map-reduce for machine learning.
\end{block}

\begin{block}{2013}
Spark becomes an Apache project and Databricks founded.
\end{block}

\begin{block}{2014}
Spark wins records for TB--PB sorting.
\end{block}
\end{frame}


%% \begin{frame}[fragile]
%% \frametitle{Map-reduce}

%% Hadoop executes two sets of independent, identical processors:
%% \begin{itemize}
%% \item Mappers, which transform each input to a $\langle$key, value$\rangle$ pair.
%% \item Reducers, each operates on all values that have a given key.
%% \end{itemize}

%% \vspace{-0.1 cm}
%% \begin{columns}
%% \column{0.5\linewidth}
%% \begin{lstlisting}[frame=single]
%% def mapper($webpage$):
%%   for $word$ in $webpage$.split():
%%     yield ($word$, $webpage$)
%% $$
%% \end{lstlisting}
%% \column{0.58\linewidth}
%% \begin{lstlisting}[frame=single]
%% def reducer($word$, $webpages$):
%%   searchIndex[$word$] = {}
%%   for $webpage$ in $webpages$:
%%     searchIndex[$word$].add($webpage$)
%% \end{lstlisting}
%% \end{columns}

%% The system groups data by key in an optimized way (independent partial sorts followed by merge, minimizing network bandwidth).

%% \vspace{0.2 cm}
%% \mbox{ } \hfill \includegraphics[width=0.6\linewidth]{mapreduce-diagram-by-ibm.png} \hfill \mbox{ }
%% \end{frame}

%% \begin{frame}[fragile]
%% \frametitle{Potential application in physics}

%% Most physics analyses apply the same function to all \mbox{events independently.\hspace{-1 cm}}

%% However, alignment and calibration are more tightly coupled:
%% \begin{itemize}
%% \item Alignment residuals must be re-indexed from tracks to subdetectors.
%% \item Calibration responses must be re-indexed from $\pi^0$s to subdetectors.
%% \end{itemize}

%% \begin{lstlisting}[frame=single]
%% def mapper($event$):
%%     for $track$ in $event$:
%%         for $hit$ in $track$:
%%             key = $hit$.subdetector()
%%             residual = $track$.projection($hit$) - $hit$.pos()
%%             yield (key, residual)
%% \end{lstlisting}

%% \begin{lstlisting}[frame=single]
%% def reducer($subdetector$, $residuals$):
%%     numer = 0
%%     denom = 0
%%     for $residual$ in $residuals$:
%%         numer = numer + $residual$
%%         denom = denom + 1
%%     >move>($subdetector$, numer/denom)  # shift by residual mean
%% \end{lstlisting}

%% Map-reduce can be applied to {\it one iteration} of alignment or calibration.
%% \end{frame}

%% \begin{frame}[fragile]
%% \frametitle{Alignment example in Spark}

%% \begin{lstlisting}
%% @case class Ratio(numer, denom):
%%     def add(self, other):
%%         return >Ratio>(self.numer + other.numer,
%%                      self.denom + other.denom)
%%     def value(self):
%%         return self.numer / self.denom

%% def makePair($track$, $hit$):
%%     key = $hit$.subdetector()
%%     residual = $track$.projection($hit$) - $hit$.pos()
%%     return (key, >Ratio>(residual, 1))
%% \end{lstlisting}

%% \begin{lstlisting}[frame=single]
%% $tracks$.cache()   # tells Spark to keep tracks in memory

%% for $iteration$ in range(100):
%%     keyValuePairs =
%%         $tracks$.flatMap(lambda $track$: $track$.hits.map(
%%                          lambda $hit$: >makePair>($track$, $hit$)))

%%     corrections =
%%         keyValuePairs.reduceByKey(lambda r1, r2: r1.>add>(r2))
%%                      .mapValues(lambda r: r.>value>())

%%     corrections.foreach(>move>)
%% \end{lstlisting}
%% \end{frame}

%% \begin{frame}
%% \frametitle{Language choice}

%% Everything mentioned so far was written in Java (or Scala or Clojure) with hooks for Python and R. Little support for C/C++. Why is that?

%% \vfill
%% \uncover<2->{\renewcommand{\arraystretch}{1.0} \begin{tabular}{>{\raggedright}p{0.3\linewidth} >{\raggedright}p{0.3\linewidth} >{\raggedright\arraybackslash}p{0.3\linewidth}}
%% \textcolor{darkblue}{C/C++} & \textcolor{darkblue}{JVM (Java et al)} & \textcolor{darkblue}{Python and R} \\
%% \vspace{-0.2 cm} Hardest to debug. Mixes analysis with low-level concerns. & \vspace{-0.2 cm} Human-readable internals, stack traces, runtime types. & \vspace{-0.2 cm} Interactive prompt! Everything can be inspected at runtime. \\
%% \uncover<3->{Fastest, raw machine access, \mbox{static bytecode,\hspace{-1 cm}} manual memory management.} & \uncover<3->{Medium speed, dynamic optimizer.} \only<3-4>{Garbage collector is fast but pauses.} \only<5>{\fbox{\begin{minipage}{\linewidth} \raggedright Garbage collector is fast but pauses. \end{minipage}}} & \uncover<3->{Slowest, though performance-critical code is external.} \\
%% \uncover<4->{Used by physicists (and cybersecurity).} & \uncover<4->{Used for large-scale business analytics; suited to networking.} & \uncover<4->{Used by statisticians and data scientists for laptop-analyses.}
%% \end{tabular}}
%% \end{frame}

%% \begin{frame}[fragile]
%% \frametitle{Immutable data}

%% Another trend is the restriction to immutable data: variables that don't vary and data structures that can only be replaced, not changed.

%% \vspace{0.5 cm}
%% \begin{columns}
%% \column{0.4\linewidth}
%% Mutable variable:

%% \begin{lstlisting}[frame=single]
%% result = 0
%% for i in range(10):
%%   result = result + i
%% >>
%% >>
%% >>
%% \end{lstlisting}

%% \vspace{0.3 cm}
%% Mutable data structure:

%% \begin{lstlisting}[frame=single]
%% result = []
%% for i in range(10):
%%   result.>append>(i)
%% >>
%% >>
%% >>
%% \end{lstlisting}

%% \column{0.5\linewidth}
%% Immutable variable:

%% \begin{lstlisting}[frame=single]
%% def add(i):
%%   if i < 10:
%%     return i + >add>(i+1)
%%   else:
%%     return 0
%% result = >add>(0)
%% \end{lstlisting}

%% \vspace{0.3 cm}
%% Immutable data structure:

%% \begin{lstlisting}[frame=single]
%% def appended(i):
%%   if i < 10:
%%     return [i] + >appended>(i+1)
%%   else:
%%     return []
%% result = >appended>(0)
%% \end{lstlisting}
%% \end{columns}
%% \end{frame}

%% \begin{frame}[fragile]
%% \frametitle{Immutable data}

%% Why impose this limitation? Because calculations are distributed.

%% \begin{itemize}
%% \item It is extremely difficult to maintain consistent values for mutable variables across computers in a network (CAP theorem).
%% \item Multiple threads in the same computer acting on a shared variable can easily corrupt it.
%% \end{itemize}

%% \vspace{0.75 cm}
%% Example: concurrent access to a mutable counter.

%% \begin{lstlisting}
%% def updateCounter():
%%     # step 1
%%     currentValue = getCounterValue()
%%     # step 2
%%     newValue = currentValue + 1
%%     # step 3
%%     setCounterValue(newValue)
%% \end{lstlisting}

%% \vspace{-4.5 cm}
%% \hfill \includegraphics[width=0.23\linewidth]{time_mutable.png}
%% \end{frame}

%% \begin{frame}[fragile]
%% \frametitle{Monoids for combining results}

%% A monoid is a group without inverses:
%% \begin{itemize}
%% \item an identity: $0$ for which $a + 0 = a$
%% \item an associative operator: $a + (b + c) = (a + b) + c$
%% \end{itemize}

%% \vspace{0.2 cm}
%% \mbox{ } \hfill \includegraphics[width=0.7\linewidth]{monoids.png} \hfill \mbox{ }

%% \vspace{0.2 cm}
%% \begin{lstlisting}
%% @case class Ratio(numer, denom):
%%     def add(self, other):
%%         return >Ratio>(self.numer + other.numer,
%%                      self.denom + other.denom)
%%     def value(self):
%%         return self.numer / self.denom
%% \end{lstlisting}
%% \end{frame}

%% \begin{frame}{Language choices for projects I was involved in}
%% \begin{itemize}
%% \item Credit card company: SAS and Python ({\it pure} Python, not even Numpy).

%% \item Web advertising start-up: Python.

%% \item NASA (open source Project Matsu): Java simply because it was a new project using Hadoop and HBase. I think they ordinarily use C++ for image processing.

%% \item Monitoring auto traffic: real-time analysis in Storm (which is in Clojure, a JVM language, but I wrote my code in Scala).

%% \item Auto insurance: SQL over Hadoop, using Hive and Pig. User-defined functions were in Java because Hive and Pig (and Hadoop) are Java.

%% \item Military project: extremely Java-centric.

%% \item Data science start-up: most data analyses in R, a little in Python, but the production data pipeline was strictly Java.
%% \end{itemize}
%% \end{frame}

%% \begin{frame}{Language choice}
%% Based on Google searches for language + ``tutorial'' (log scale).

%% \includegraphics[width=\linewidth]{language_trends.png}
%% \end{frame}

%% \begin{frame}{Major frameworks}
%% \begin{block}{Apache Hadoop}
%% Performs map-reduce calculations. Used as a foundation for other big data frameworks because of\ldots
%% \begin{itemize}
%% \item the HDFS distributed filesystem (even variants like MapR, which don't use HDFS, provide an HDFS API),
%% \item suite of InputFormats that split files by logical records,
%% \item ZooKeeper, which coordinates job configuration and synchronization for any service across a cluster.
%% \end{itemize}
%% \end{block}

%% \begin{block}{Apache Spark}
%% Generalizes from map-reduce to arbitrary pipelines, optimized for iterative procedures, with an interactive prompt. May be used on any cluster manager, but usually Hadoop.
%% \begin{itemize}
%% \item User interfaces: native Scala, Java, Python (through sockets), and R (through pipes).
%% \end{itemize}
%% \end{block}
%% \end{frame}

%% \begin{frame}[fragile]{Spark's functional primitives in \only<1>{C++}\only<2>{Scala} type syntax}
%% \small
%% \begin{onlyenv}<1>
%% \begin{verbatim}
%% iterator<X> filter(iterator<X>, function<bool(X)>);
%% iterator<Y> map(iterator<X>, function<Y(X)>);
%% iterator<Y> flatten(iterator<iterator<Y>>);
%% iterator<Y> flatMap(iterator<X>, function<iterator<Y>(X)>);

%% Y reduce(iterator<Y>, function<Y(Y,Y)>);      // sum, max, etc.
%% Z aggregate(iterator<Y>, Z, function<Z(Y,Z)>);   // more general

%% map<K,array<V>> groupByKey(iterator<pair<K,V>>); // like SQL's

%% map<K,V> reduceByKey(iterator<pair<K,V>>,     // input data
%%                      function<V(V,V)>);       // merge

%% map<K,Z> aggregateByKey(iterator<pair<K,V>>,  // input data
%%                         Z,                    // starting value
%%                         function<Z(V,Z)>,     // increment
%%                         function<Z(Z,Z)>);    // combine
%% \end{verbatim}
%% \end{onlyenv}
%% \begin{onlyenv}<2>
%% \begin{verbatim}
%% def filter(in: Iterator[X], f: X => Boolean): Iterator[X]
%% def map(in: Iterator[X], f: X => Y): Iterator[Y]
%% def flatten(in: Iterator[Iterator[Y]]): Iterator[Y]
%% def flatMap(in: Iterator[X], f: X => Iterator[Y]): Iterator[Y]

%% def reduce(in: Iterator[Y], f: (Y,Y) => Y): Y
%% def aggregate(in: Iterator[Y], zero: Z, f: (Y,Z) => Z): Z

%% def groupByKey(in: Iterator[(K,V)]): Map[K,Seq[V]]

%% def reduceByKey(in: Iterator[(K,V)],
%%                 merge: (V,V) => V): Map[K,V]

%% def aggregateByKey(in: Iterator[(K,V)],
%%                    zero: Z,
%%                    incr: (V,Z) => Z,
%%                    comb: (Z,Z) => Z): Map[K,Z]
%% \end{verbatim}
%% \end{onlyenv}

%% \normalsize
%% Hadoop's ``mappers'' are actually {\tt flatMap} (which includes the possibility of filtering) and its ``reducers'' are {\tt aggregateByKey}.
%% \end{frame}

%% \begin{frame}[fragile]{Example of using primitives}
%% Suppose we want to fill histograms of $p_T$ for tracks with {\tt fValid} from events with {\tt fTemperature > 20}, with one histogram for each distinct combination of {\tt fTriggerBits}.

%% \begin{onlyenv}<1>
%% \vspace{0.2 cm}
%% On the Spark prompt (Scala syntax):
%% \begin{verbatim}
%% val dataset = sc.rootRDD[Event]("root://fnal.gov/*.root")
%% val histograms =
%%   dataset.filter(event => event.fTemperature > 20)
%%          .flatMap(event => event.fTracks)
%%          .filter(track => track.fValid)
%%          .map(t => (t.fTriggerBits,
%%                     Math.sqrt(t.fPx*t.fPx + t.fPy*t.fPy)))
%%          .aggregateByKey(new TH1F(100, 0, 30))(
%%                          (pt, hist) => hist.Fill(pt),
%%                          (h1, h2) => h1.Add(h2))
%% \end{verbatim}
%% \end{onlyenv}
%% \begin{onlyenv}<2>
%% \vspace{0.2 cm}
%% On the PySpark prompt (Python syntax):
%% \begin{verbatim}
%% dataset = sc.rootRDD("root://fnal.gov/*.root")
%% histograms =
%%   dataset.filter(lambda event: event.fTemperature > 20)
%%          .flatMap(lambda event: event.fTracks)
%%          .filter(lambda track: track.fValid)
%%          .map(lambda t: (t.fTriggerBits,
%%                          math.sqrt(t.fPx**2 + t.fPy**2)))
%%          .aggregateByKey(TH1F(100, 0, 30),
%%                          lambda pt, hist: hist.Fill(pt),
%%                          lambda h1, h2: h1.Add(h2))
%% \end{verbatim}
%% \end{onlyenv}

%% This process gets distributed over the cluster and returns its result to the user's \only<1>{Spark}\only<2>{PySpark} interactive session.
%% \end{frame}

%% KDNuggets
%% Hadoop/Big Data Tools

%% Hadoop/Big Data tool usage jumped to 29% among voters, up from 17% in 2014, and 14% in 2013.
%% This is probably due to availability and low-cost of many cloud-based Big Data tools. Very notable is the jump in Spark share to 11.3%.

%% However, most data analysis is still done on "medium" and small data.

%% Top Hadoop/Big Data tools were

%% Hadoop, 18.4% share (507 votes)
%% Spark, 11.3% (311)
%% Hive, 10.2% (282)
%% SQL on Hadoop tools, 7.2% (198)
%% Pig, 5.4% (150)
%% HBase, 4.6% (127)
%% Other Hadoop/HDFS-based tools, 4.5% (125)
%% MLlib, 3.3% (91)
%% Mahout, 2.8% (76)
%% Datameer, 0.8% (23)

\end{document}
